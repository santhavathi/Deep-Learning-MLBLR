{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CommonSense.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/santhavathi/Deep-Learning-MLBLR/blob/master/CommonSense.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "3CN0Se_661tt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Three-way Attention and Relational Knowledge for Commonsense Machine Comprehension"
      ]
    },
    {
      "metadata": {
        "id": "zWdzQuE8KOvD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Research Paper and Pytorch Code\n",
        "This paper discusses about machine comprehension using commonsense knowledge \n",
        "\n",
        "[Research Paper](https://arxiv.org/pdf/1803.00191.pdf)\n",
        "\n",
        "[Pytorch implementation](https://github.com/intfloat/commonsenserc/tree/ed4d40d20eabf7788c56366fe0e224287db0d383)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JtI6ntcAKXqo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Objective\n",
        "1. The objective of this paper is to train the machine to learn from a sequence of English passages and predict whether the anwser to the question based on the passage is right or not.\n",
        "\n",
        "2. The dataset is a corpus of passage text, each passage text has one or more questions and each question has a choice of 4 answers of which one is the right answer.\n",
        "\n",
        "3. So input to the model is a passage text + question + answer and output is a label 0/1 representing wrong/right answer.\n",
        "\n",
        "4. Model should predict if the answer for a question based on a passage is right or not.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2gx7hCJwKgZe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Model\n",
        "\n",
        "**Input Layer**\n",
        "\n",
        "300 dimensional glove embedding + 12 dimensional part-of-speech embedding + 8 dimensional named-entity embedding + 10 dimensional relation embedding from Conceptnet\n",
        "\n",
        "**Attention Layers**\n",
        "\n",
        "Question-aware passage representation<br>\n",
        "Passage-aware answer representation<br>\n",
        "Question-aware answer representation<br>\n",
        "\n",
        "**LSTM Layers**\n",
        "\n",
        "Three BiLSTMs are applied to the concatenation of the above attention layer vectors to model the temporal dependency\n",
        "\n",
        "**Attention Layers**\n",
        "\n",
        "3 Self attention layers one each for the above 3 LSTM layers \n",
        "\n",
        "**Output Layer**\n",
        "\n",
        "Sigmoid of the Bilinear interaction of previous attention layers\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "lvSBrofvmv9r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Highlights"
      ]
    },
    {
      "metadata": {
        "id": "Nsd2uia9m_CS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Threeway Attentive Networks (TriAN) is used to model interactions between the passage, question and answers.\n",
        "* Different questions need to focus on different parts of the passage, attention mechanism is a natural choice and turns out to be effective for reading comprehension.\n",
        "* To incorporate commonsense knowledge, input is augmented with relation embedding from the graph of general knowledge ConceptNet which improved accuracy by 1%\n",
        "* ConceptNet consists of over 21 million edges and 8 million nodes and shows state-of-the-art performance on tasks like word analogy and word relatedness.\n",
        "* State-of-the-art performance with 83.95% accuracy on test data and 85.27% accuracy on dev data.\n",
        "* First pretrained on RACE dataset for 10 epochs which improved accuracy by 1%\n",
        "* Model TriAN was implemented based on PyTorch 4.\n",
        "* Models were trained on a single GPU(Tesla P40) and each epoch took about 80 seconds. \n",
        "* Only the word embeddings of top 10 frequent words are fine-tuned during training.\n",
        "* One layer of LSTM is used.\n",
        "* The dimension of both forward and backward LSTM hidden state is set to 96. \n",
        "* Dropout rate is set to 0.4 for both input embeddings and BiLSTM outputs. \n",
        "* For parameter optimization, Adamax was used with an initial learning rate 2 × 10−3.\n",
        "* Learning rate is then halved after 10 and 15 training epochs. \n",
        "* The model converges after 50 epochs.\n",
        "* Gradients are clipped to have a maximum L2 norm of 10. \n",
        "* Minibatch with batch size 32 is used.\n",
        "* Hyperparameters are optimized by random search strategy.\n",
        "* Standard cross entropy function is used as the loss function to minimize.\n"
      ]
    },
    {
      "metadata": {
        "id": "AhHsffG8Kl3H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Detailed Explanation\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3XZT21AJKr54",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Input Layer\n",
        "\n",
        "**Passage** = p_inp => **Shape**(32 x 900) \n",
        "batch_size=32, Max(length of words of each of 32 passages)=900, Each of the 900 words is represented by its normalised value\n",
        "\n",
        "**Passage Mask** = p_mask_inp => Shape(32 x 900)\n",
        "batch_size=32, For passages where the length is < 900 the corresponding empty word value will be set to \"0\"\n",
        "\n",
        "**Passage Part of Speech** = p_pos_inp => Shape(32 x 900)\n",
        "batch_size=32, for each of the 900 words its part of speech is represented by a normalised value. Examples of Part of Speech are Noun, Verb, etc\n",
        "\n",
        "**Passage Named Entity Relationship** = p_ner_inp => Shape(32 x 900)\n",
        "batch_size=32, for each of the 900 words its named entity relationship is represented by a normalised value. Examples of Named Entity Relationship are Product, RelatesTo, Date, Person, etc\n",
        "\n",
        "**Question** = q_inp => Shape(32 x 25)\n",
        "batch_size=32, Max(length of words of each of 32 questions)=25, Each of the 25 words is represented by its normalised value\n",
        "\n",
        "**Question Mask** = q_mask_inp => Shape(32 x 25)\n",
        "batch_size=32, For questions where the length is < 25 the corresponding empty word value will be set to \"0\"\n",
        "\n",
        "**Question Part of Speech** = q_pos_inp => Shape(32 x 25)\n",
        "batch_size=32, for each of the 25 words its part of speech is represented by a normalised value. Examples of Part of Speech are Noun, Verb, etc\n",
        "\n",
        "**Answer/Choice** = c_inp => Shape(32 x 35)\n",
        "batch_size=32, Max(length of words of each of 32 answers)=35, Each of the 35 words is represented by its normalised value\n",
        "\n",
        "**Choice Mask** = c_mask_inp => Shape(32 x 35)\n",
        "batch_size=32, For answers where the length is < 35 the corresponding empty word value will be set to \"0\"\n",
        "\n",
        "**Handcrafted Features** = f_tensor_inp => Shape(32 x 900 x 5)\n",
        "batch_size=32, 5 handcrafted features(in_q, in_c, lemma_in_q, lemma_in_c, tf) for each of the 900 words\n",
        "in_q = 1 if passage word is present in question, else 0\n",
        "in_c = 1 if passage word is present in answer, else 0\n",
        "lemma_in_q = 1 if lemmatised passage word is present in question, else 0 (for, eg. sitting is a lemmatised word for sit)\n",
        "lema_in_c = 1 if lemmatised passage word is present in answer, else 0\n",
        "tf = term frequency representation of each word\n",
        "\n",
        "**Passage Question Relationship** = p_q_rel_inp => Shape(32 x 900)\n",
        "batch_size=32, if there is a relationship of each of the 900 words with the question, this is from conceptnet model\n",
        "\n",
        "**Passage Choice Relationship** = p_c_rel_inp => Shape(32 x 900)\n",
        "batch_size=32, if there is a relationship of each of the 900 words with the answer, this is from conceptnet model\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_j6dqCRfK1G7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Common Sense Core Layers\n",
        "\n",
        "**Step 1 - Embeddings**\n",
        "\n",
        "Create following embeddings\n",
        "\n",
        "p_emb = $E^{glove}_{{P}_{i}}$ <br>\n",
        "q_emb = $E^{glove}_{{Q}_{i}}$ <br>\n",
        "c_emb = $E^{glove}_{{A}_{i}}$ <br>\n",
        "p_pos_emb = $E^{pos}_{{P}_{i}}$ <br>\n",
        "q_pos_emb = $E^{pos}_{{Q}_{i}}$ <br>\n",
        "p_ner_emb = $E^{ner}_{{P}_{i}}$ <br>\n",
        "p_q_rel_emb = $E^{rel}_{{P}_{i},\\{{Q}_{i}\\}^{|Q|}_{i=1}}$ <br>\n",
        "p_c_rel_emb = $E^{rel}_{{P}_{i},\\{{A}_{i}\\}^{|A|}_{i=1}}$ <br>\n",
        "\n",
        "p_emb(32 x 900 x 300), q_emb(32 x 25 x 300), c_emb(32 x 35 x 300) => 300 vector Glove embedding for Passage, Question, Answer <br>\n",
        "p_pos_emb(32 x 900 x 12), q_pos_emb(32 x 25 x 12) => 12 vector Part of Speech embedding for Passage, Question <br>\n",
        "p_ner_emb(32 x 900 x 8) => 8 vector Named Entity Relationship embedding for Passage <br>\n",
        "p_q_rel_emb(32 x 900 x 10), p_c_rel_emb(32 x 900 x 10) => 10 vector Relationship embedding from Conceptnet for Passage+Question and Passage+Answer <br>\n",
        "\n",
        "**Step 2 - Sequential Attention Layer**\n",
        "\n",
        "${Att}_{seq}(u, \\{v_i\\}^n_{i=1}) = {\\sum_{i=1}^n} {\\alpha}_i v_i$ <br>\n",
        "${\\alpha}_i = {softmax}_i(f(W_1u)^T f(W_1v_i))$ => where f=ReLU <br>\n",
        "\n",
        "Question-aware passage representation => p_q_weighted_emb(32 x 900 x 300)<br>\n",
        "Paying attention to the passage with respect to the question<br>\n",
        "$\\{w^q_{P_i}\\}^{|P|}_{i=1}$ <br>\n",
        "$w^q_{P_i} = {Att}_{seq}(E^{glove}_{{P}_{i}},\\{E^{glove}_{{Q}_{i}}\\}^{|Q|}_{i=1})$ <br>\n",
        "\n",
        "Passage-aware answer representation => c_p_weighted_emb(32 x 35 x 300)<br>\n",
        "Paying attention to the passage with respect to the answer<br>\n",
        "$\\{w^p_{A_i}\\}^{|A|}_{i=1}$ <br>\n",
        "$w^p_{A_i} = {Att}_{seq}(E^{glove}_{{A}_{i}},\\{E^{glove}_{{P}_{i}}\\}^{|P|}_{i=1})$ <br>\n",
        "\n",
        "Question-aware answer representation => c_q_weighted_emb(32 x 35 x 300)<br>\n",
        "Paying attention to the question with respect to the answer<br>\n",
        "$\\{w^q_{A_i}\\}^{|A|}_{i=1}$ <br>\n",
        "$w^q_{A_i} = {Att}_{seq}(E^{glove}_{{A}_{i}},\\{E^{glove}_{{Q}_{i}}\\}^{|Q|}_{i=1})$ <br>\n",
        "\n",
        "\n",
        "**Step 3 - Bidirectional LSTMs**\n",
        "\n",
        "2 * hidden_layer_size= 2 * 96 = 192<br>\n",
        "$h^q = BiLSTM(\\{wQ_i\\}^{|Q|}_{i=1})$ => q_hiddens(32 x 25 x 192)<br>\n",
        "$h^p = BiLSTM(\\{ [w_{P_i};  w^q_{P_i}] \\}^{|P|}_{i=1})$ => p_hiddens(32 x 900 x 192)<br>\n",
        "$h^a = BiLSTM(\\{ [w_{A_i};  w^p_{A_i}; w^q_{A_i}] \\}^{|A|}_{i=1})$ => a_hiddens(32 x 35 x 192)<br>\n",
        "\n",
        "**Step 4 - Self Attention Layers**\n",
        "\n",
        "LinearSeqAttLayer - Question representation => q_merge_weights(32 x 25)<br>\n",
        "${Att}_{self}(\\{h_i^q\\}^{|Q|}_{i=1})$<br>\n",
        "\n",
        "LinearSeqAttLayer - Answer representation => c_merge_weights(32 x 35)<br>\n",
        "${Att}_{self}(\\{h_i^a\\}^{|A|}_{i=1})$<br>\n",
        "\n",
        "BiLinearSeqAttLayer - Passage representation => p_merge_weights(32 x 900)<br>\n",
        "${Att}_{self}(q,\\{h_i^p\\}^{|P|}_{i=1})$<br>\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JbkErF_lK-Ls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Output Layer\n",
        "Output Layer => preds(32 x 1) where 1 represents the probability (0 or 1)<br>\n",
        "$y = \\sigma_(p^TW_3a + q^TW_4a)$ \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "czYw5EojQluX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Input Files Needed\n",
        "[preprocessed.zip](https://drive.google.com/open?id=1M1saVYk-4Xh0Y0Ok6e8liDLnElnGc0P4)\n",
        "\n",
        "[rel_vocab](https://github.com/intfloat/commonsense-rc/blob/master/data/rel_vocab)"
      ]
    },
    {
      "metadata": {
        "id": "irSiA0o7RgGe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Code Implementation in Keras"
      ]
    },
    {
      "metadata": {
        "id": "WtBkDIvhRtl1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Download and Unzip Input Files"
      ]
    },
    {
      "metadata": {
        "id": "6bKVZZxLpWSG",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7bdfb44d-3ee1-4bd9-8f97-434283d91d79"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-936b29de-8023-4507-9075-e1e87c601066\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-936b29de-8023-4507-9075-e1e87c601066\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving preprocessed.zip to preprocessed.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KkVJiuRSwGvR",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c6f948d6-9879-408e-df76-001807964df6"
      },
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ec401767-76fa-4355-b4d7-caa02e982fe3\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ec401767-76fa-4355-b4d7-caa02e982fe3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving rel_vocab to rel_vocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2LDBJqCTvMhQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "1a6fe189-8bcf-4bf2-db0f-eaeeb15516fd"
      },
      "cell_type": "code",
      "source": [
        "!unzip preprocessed.zip\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  preprocessed.zip\n",
            "  inflating: concept.filter          \n",
            "  inflating: dev-data-processed.json  \n",
            "  inflating: test-data-processed.json  \n",
            "  inflating: train-data-processed.json  \n",
            "  inflating: trial-data-processed.json  \n",
            "  inflating: glove.840B.300d.txt     \n",
            "/content\n",
            "concept.filter\t\t glove.840B.300d.txt  test-data-processed.json\n",
            "datalab\t\t\t preprocessed.zip     train-data-processed.json\n",
            "dev-data-processed.json  rel_vocab\t      trial-data-processed.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VC3qR0dTR2CN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Import modules"
      ]
    },
    {
      "metadata": {
        "id": "mDtFeOQKLgNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "87f73e79-ca16-4cf3-f473-8a47e0f6f80d"
      },
      "cell_type": "code",
      "source": [
        "!python -c 'import keras; print(keras.__version__)'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zZfTSZKDouTV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import string\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5woLZwLqo2Kb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22039ff0-fe4b-4f4f-c849-7c36e9ed44af"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Bidirectional, LSTM, Concatenate, Dense, Input, Embedding, Dropout, merge, Activation, Lambda, Flatten\n",
        "from keras.layers import Add, Multiply, Reshape\n",
        "from keras.optimizers import Adamax, SGD\n",
        "from keras.initializers import TruncatedNormal\n",
        "from keras.engine.topology import Layer\n",
        "from keras import backend as K"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "753XfLBCSBKd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Data preparation\n"
      ]
    },
    {
      "metadata": {
        "id": "P1nXgsC9STLc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Load trial, train, dev, test data from files"
      ]
    },
    {
      "metadata": {
        "id": "CKAcw8dno5q6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Dictionary():\n",
        "    NULL = '<NULL>'\n",
        "    UNK = '<UNK>'\n",
        "    START = 2\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize(token):\n",
        "        return unicodedata.normalize('NFD', token)\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tok2ind = {self.NULL: 0, self.UNK: 1}\n",
        "        self.ind2tok = {0: self.NULL, 1: self.UNK}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tok2ind)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.tok2ind)\n",
        "\n",
        "    def __contains__(self, key):\n",
        "        if type(key) == int:\n",
        "            return key in self.ind2tok\n",
        "        elif type(key) == str:\n",
        "            return self.normalize(key) in self.tok2ind\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if type(key) == int:\n",
        "            return self.ind2tok.get(key, self.UNK)\n",
        "        if type(key) == str:\n",
        "            return self.tok2ind.get(self.normalize(key),\n",
        "                                    self.tok2ind.get(self.UNK))\n",
        "\n",
        "    def __setitem__(self, key, item):\n",
        "        if type(key) == int and type(item) == str:\n",
        "            self.ind2tok[key] = item\n",
        "        elif type(key) == str and type(item) == int:\n",
        "            self.tok2ind[key] = item\n",
        "        else:\n",
        "            raise RuntimeError('Invalid (key, item) types.')\n",
        "\n",
        "    def add(self, token):\n",
        "        token = self.normalize(token)\n",
        "        if token not in self.tok2ind:\n",
        "            index = len(self.tok2ind)\n",
        "            self.tok2ind[token] = index\n",
        "            self.ind2tok[index] = token\n",
        "\n",
        "    def tokens(self):\n",
        "        \"\"\"Get dictionary tokens.\n",
        "\n",
        "        Return all the words indexed by this dictionary, except for special\n",
        "        tokens.\n",
        "        \"\"\"\n",
        "        tokens = [k for k in self.tok2ind.keys()\n",
        "                  if k not in {'<NULL>', '<UNK>'}]\n",
        "        return tokens\n",
        "    \n",
        "vocab, pos_vocab, ner_vocab, rel_vocab = Dictionary(), Dictionary(), Dictionary(), Dictionary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4CpHKuOwo6cs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Example:\n",
        "\n",
        "    def __init__(self, input_dict):\n",
        "        self.id = input_dict['id']\n",
        "        self.passage = input_dict['d_words']\n",
        "        self.question = input_dict['q_words']\n",
        "        self.choice = input_dict['c_words']\n",
        "        self.d_pos = input_dict['d_pos']\n",
        "        self.d_ner = input_dict['d_ner']\n",
        "        self.q_pos = input_dict['q_pos']\n",
        "        assert len(self.q_pos) == len(self.question.split()), (self.q_pos, self.question)\n",
        "        assert len(self.d_pos) == len(self.passage.split())\n",
        "        self.features = np.stack([input_dict['in_q'], input_dict['in_c'], \\\n",
        "                                    input_dict['lemma_in_q'], input_dict['lemma_in_c'], \\\n",
        "                                    input_dict['tf']], 1)\n",
        "        assert len(self.features) == len(self.passage.split())\n",
        "        self.label = input_dict['label']    \n",
        "     \n",
        "        self.d_tensor = np.array([vocab[w] for w in self.passage.split()])\n",
        "        self.q_tensor = np.array([vocab[w] for w in self.question.split()])\n",
        "        self.c_tensor = np.array([vocab[w] for w in self.choice.split()])\n",
        "        self.d_pos_tensor = np.array([pos_vocab[w] for w in self.d_pos])\n",
        "        self.q_pos_tensor = np.array([pos_vocab[w] for w in self.q_pos])\n",
        "        self.d_ner_tensor = np.array([ner_vocab[w] for w in self.d_ner])\n",
        "        self.p_q_relation = np.array([rel_vocab[r] for r in input_dict['p_q_relation']])\n",
        "        self.p_c_relation = np.array([rel_vocab[r] for r in input_dict['p_c_relation']])\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'Passage: %s\\n Question: %s\\n Answer: %s, Label: %d' % (self.passage, self.question, self.choice, self.label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CkzM1GiVo81A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    #from doc import Example\n",
        "    i=0\n",
        "    data = []\n",
        "    for line in open(path, 'r', encoding='utf-8'):\n",
        "        #i=i+1\n",
        "        #if i > 7:\n",
        "            #break\n",
        "        if path.find('race') < 0 or np.random.random() < 0.6:\n",
        "            data.append(Example(json.loads(line)))\n",
        "    print('Load %d examples from %s...' % (len(data), path))\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vGDN-W9SI4f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Build vocab file\n",
        "Vocab file contains all the distinct words in the entire corpus of passage, questions, answers"
      ]
    },
    {
      "metadata": {
        "id": "HSDwHN4Do_No",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_vocab(data=None):\n",
        "    global vocab, pos_vocab, ner_vocab, rel_vocab\n",
        "    # build word vocabulary\n",
        "    if os.path.exists('./vocab'):\n",
        "        print('Load vocabulary from ./vocab...')\n",
        "        for w in open('./vocab', encoding='utf-8'):\n",
        "            vocab.add(w.strip())\n",
        "        print('Vocabulary size: %d' % len(vocab))\n",
        "    else:\n",
        "        cnt = Counter()\n",
        "        for ex in data:\n",
        "            cnt += Counter(ex.passage.split())\n",
        "            cnt += Counter(ex.question.split())\n",
        "            cnt += Counter(ex.choice.split())\n",
        "        for key, val in cnt.most_common():\n",
        "            vocab.add(key)\n",
        "        print('Vocabulary size: %d' % len(vocab))\n",
        "        writer = open('./vocab', 'w', encoding='utf-8')\n",
        "        writer.write('\\n'.join(vocab.tokens()))\n",
        "        writer.close()\n",
        "    # build part-of-speech vocabulary\n",
        "    if os.path.exists('./pos_vocab'):\n",
        "        print('Load pos vocabulary from ./pos_vocab...')\n",
        "        for w in open('./pos_vocab', encoding='utf-8'):\n",
        "            pos_vocab.add(w.strip())\n",
        "        print('POS vocabulary size: %d' % len(pos_vocab))\n",
        "    else:\n",
        "        cnt = Counter()\n",
        "        for ex in data:\n",
        "            cnt += Counter(ex.d_pos)\n",
        "            cnt += Counter(ex.q_pos)\n",
        "        for key, val in cnt.most_common():\n",
        "            if key: pos_vocab.add(key)\n",
        "        print('POS vocabulary size: %d' % len(pos_vocab))\n",
        "        writer = open('./pos_vocab', 'w', encoding='utf-8')\n",
        "        writer.write('\\n'.join(pos_vocab.tokens()))\n",
        "        writer.close()\n",
        "    # build named entity vocabulary\n",
        "    if os.path.exists('./ner_vocab'):\n",
        "        print('Load ner vocabulary from ./ner_vocab...')\n",
        "        for w in open('./ner_vocab', encoding='utf-8'):\n",
        "            ner_vocab.add(w.strip())\n",
        "        print('NER vocabulary size: %d' % len(ner_vocab))\n",
        "    else:\n",
        "        cnt = Counter()\n",
        "        for ex in data:\n",
        "            cnt += Counter(ex.d_ner)\n",
        "        for key, val in cnt.most_common():\n",
        "            if key: ner_vocab.add(key)\n",
        "        print('NER vocabulary size: %d' % len(ner_vocab))\n",
        "        writer = open('./ner_vocab', 'w', encoding='utf-8')\n",
        "        writer.write('\\n'.join(ner_vocab.tokens()))\n",
        "        writer.close()\n",
        "    # Load conceptnet relation vocabulary\n",
        "    assert os.path.exists('./rel_vocab')\n",
        "    print('Load relation vocabulary from ./rel_vocab...')\n",
        "    for w in open('./rel_vocab', encoding='utf-8'):\n",
        "        rel_vocab.add(w.strip())\n",
        "    print('Rel vocabulary size: %d' % len(rel_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WMMutUt4pBOV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "aa1391f9-0c52-4a9b-c4d2-fe4da88de5da"
      },
      "cell_type": "code",
      "source": [
        "trial_data = load_data('./trial-data-processed.json')\n",
        "train_data = load_data('./train-data-processed.json')\n",
        "dev_data = load_data('./dev-data-processed.json')\n",
        "test_data = load_data('./test-data-processed.json')\n",
        "build_vocab(trial_data + train_data + dev_data + test_data)\n",
        "#build_vocab(trial_data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load 1020 examples from ./trial-data-processed.json...\n",
            "Load 19462 examples from ./train-data-processed.json...\n",
            "Load 2822 examples from ./dev-data-processed.json...\n",
            "Load 5594 examples from ./test-data-processed.json...\n",
            "Vocabulary size: 12718\n",
            "POS vocabulary size: 51\n",
            "NER vocabulary size: 20\n",
            "Load relation vocabulary from ./rel_vocab...\n",
            "Rel vocabulary size: 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PVlFXejPpGNm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "717c892a-9d5b-4fc1-a3bb-65c51f382e31"
      },
      "cell_type": "code",
      "source": [
        "#Need to run this block again, because load_data and build_vocab are inter dependent\n",
        "#May need to change the code to overcome this cyclic dependency\n",
        "trial_data = load_data('./trial-data-processed.json')\n",
        "train_data = load_data('./train-data-processed.json')\n",
        "dev_data = load_data('./dev-data-processed.json')\n",
        "test_data = load_data('./test-data-processed.json')\n",
        "build_vocab(trial_data + train_data + dev_data + test_data)\n",
        "#build_vocab(trial_data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load 1020 examples from ./trial-data-processed.json...\n",
            "Load 19462 examples from ./train-data-processed.json...\n",
            "Load 2822 examples from ./dev-data-processed.json...\n",
            "Load 5594 examples from ./test-data-processed.json...\n",
            "Load vocabulary from ./vocab...\n",
            "Vocabulary size: 12718\n",
            "Load pos vocabulary from ./pos_vocab...\n",
            "POS vocabulary size: 51\n",
            "Load ner vocabulary from ./ner_vocab...\n",
            "NER vocabulary size: 20\n",
            "Load relation vocabulary from ./rel_vocab...\n",
            "Rel vocabulary size: 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "abYpZFNOSr3S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Sample Input Data"
      ]
    },
    {
      "metadata": {
        "id": "nxM9nqVNpG6t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "e202b86a-1960-4a20-f9e0-e1aa1f586698"
      },
      "cell_type": "code",
      "source": [
        "print(np.shape(trial_data))\n",
        "print(len(trial_data))\n",
        "print(trial_data[0])\n",
        "print(trial_data[0].passage)\n",
        "print(trial_data[2].c_tensor)\n",
        "print(type(trial_data[2].c_tensor))\n",
        "print(len(trial_data[2].c_tensor))\n",
        "print(np.shape(trial_data[2].features))\n",
        "print(trial_data[2].d_tensor)\n",
        "print(trial_data[2].p_q_relation)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1020,)\n",
            "1020\n",
            "Passage: I began by finding the culprit of my flat tire , it was a nail on the side of the road . Unfortunately I did not have a spare tire so I had to carry my bike the rest of the way home . After the gruesome trek back to my house I flipped the bicycle upside down and began my repair . First I took off the outer tire to replace my tube with a better more expensive tube that has the ability to fix a puncture . I do not want to experience carrying my bike a mile ever again . After putting the new tube on I screw back on the tire and put the outer tire over the tube . I then fill the PSI to 170 in my tire and listen for any leaking air . Then I spin the tire to make all the slime inside become evenly distributed throughout . This will prevent leaks in the future and even with a slight leak I will still be able to ride back home and fix the tire quickly yet again .\n",
            " Question: how often does their bike have problems like that ?\n",
            " Answer: every day, Label: 0\n",
            "I began by finding the culprit of my flat tire , it was a nail on the side of the road . Unfortunately I did not have a spare tire so I had to carry my bike the rest of the way home . After the gruesome trek back to my house I flipped the bicycle upside down and began my repair . First I took off the outer tire to replace my tube with a better more expensive tube that has the ability to fix a puncture . I do not want to experience carrying my bike a mile ever again . After putting the new tube on I screw back on the tire and put the outer tire over the tube . I then fill the PSI to 170 in my tire and listen for any leaking air . Then I spin the tire to make all the slime inside become evenly distributed throughout . This will prevent leaks in the future and even with a slight leak I will still be able to ride back home and fix the tire quickly yet again .\n",
            "[120   8 121]\n",
            "<class 'numpy.ndarray'>\n",
            "3\n",
            "(187, 5)\n",
            "[  3  16  28  29   2  30  11   7  31   5  32  33  34   8  35  12   2  36\n",
            "  11   2  37   4  38   3  39  15  25   8  40   5  41   3  42   6  43   7\n",
            "  14   2  44  11   2  45  17   4  18   2  46  47  13   6   7  48   3  49\n",
            "   2  27  50  51   9  16   7  52   4  53   3  54  55   2  19   5   6  56\n",
            "   7  10  20   8  57  58  59  10  26  60   2  61   6  21   8  62   4   3\n",
            "  63  15  64   6  65  66   7  14   8  67  68  22   4  18  69   2  70  10\n",
            "  12   3  71  13  12   2   5   9  72   2  19   5  73   2  10   4   3  74\n",
            "  75   2  76   6  77  23   7   5   9  78  79  80  81  82   4  83   3  84\n",
            "   2   5   6  85  86   2  87  88  89  90  91  92   4  93  24  94  95  23\n",
            "   2  96   9  97  20   8  98  99   3  24 100 101 102   6 103  13  17   9\n",
            "  21   2   5 104 105  22   4]\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 32  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 32  0  0 32 32  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0 35  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0 32  0 32  0  0  0  0 32  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xLXhDq0pwmkP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "bcb8fa26-ceb2-4eb6-ff0c-44f71d255789"
      },
      "cell_type": "code",
      "source": [
        "print(len(vocab.tokens()))\n",
        "print('cake' in vocab)\n",
        "print(vocab['cake'])\n",
        "print(vocab.normalize('cake'))\n",
        "#{w for w in vocab.tokens() if w in vocab}"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128\n",
            "False\n",
            "1\n",
            "cake\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zelre8zKSyGy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Find maximum length of para, question, choice"
      ]
    },
    {
      "metadata": {
        "id": "TO9FKpqWBqI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2a65d2f9-7c61-4d71-c7fd-c4cd0cc09ed8"
      },
      "cell_type": "code",
      "source": [
        "len1=[]\n",
        "for i in train_data:\n",
        "  len1.append(len(i.passage.split()))\n",
        "print(max(len1))\n",
        "\n",
        "len1=[]\n",
        "for i in trial_data:\n",
        "  len1.append(len(i.question.split()))\n",
        "print(max(len1))\n",
        "\n",
        "len1=[]\n",
        "for i in test_data:\n",
        "  len1.append(len(i.choice.split()))\n",
        "print(max(len1))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "860\n",
            "19\n",
            "31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NgUYZstZ1mM2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Choose a length that is higher than all the input data\n",
        "#It is recommended to have fixed size inputs for LSTM, than vaiable input shapes\n",
        "#The missing inputs are filled with zeros\n",
        "#For eg., if passage lenth of an input row is less than 900 words, the remaining words are represented by zeros\n",
        "MAX_PARA_LENGTH = 900\n",
        "MAX_QUESTION_LENGTH = 25\n",
        "MAX_CHOICE_LENGTH = 35\n",
        "HANDCRAFTED_FEATURES = 5\n",
        "\n",
        "batch_size=32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CkYUjdaIS9ob",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Load Glove Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "TjgKj09LwpSh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_embeddings(words, embedding_file):\n",
        "        \"\"\"Load pretrained embeddings for a given list of words, if they exist.\n",
        "        Args:\n",
        "            words: iterable of tokens. Only those that are indexed in the\n",
        "              dictionary are kept.\n",
        "            embedding_file: path to text file of embeddings, space separated.\n",
        "        \"\"\"\n",
        "        words = {w for w in words if w in vocab}\n",
        "        print('Loading pre-trained embeddings for %d words from %s' %(len(words), embedding_file))\n",
        "        #embedding = self.network.embedding.weight.data\n",
        "\n",
        "        # When normalized, some words are duplicated. (Average the embeddings).\n",
        "        vec_counts = {}\n",
        "        embedding = {}\n",
        "        with open(embedding_file) as f:\n",
        "            for line in f:\n",
        "                parsed = line.rstrip().split(' ')\n",
        "                assert(len(parsed) == embedding_dim + 1)\n",
        "                w = vocab.normalize(parsed[0])\n",
        "                if w in words:\n",
        "                    #vec = torch.Tensor([float(i) for i in parsed[1:]])\n",
        "                    vec = np.array([float(i) for i in parsed[1:]])\n",
        "                    if w not in vec_counts:\n",
        "                        vec_counts[w] = 1\n",
        "                        embedding[vocab[w]] = vec\n",
        "                    else:\n",
        "                        print('WARN: Duplicate embedding found for %s' % w)\n",
        "                        vec_counts[w] = vec_counts[w] + 1\n",
        "                        embedding[vocab[w]] = np.add(embedding[vocab[w]],vec)\n",
        "\n",
        "        for w, c in vec_counts.items():\n",
        "            embedding[vocab[w]] = embedding[vocab[w]]/c\n",
        "\n",
        "        print('Loaded %d embeddings (%.2f%%)' %(len(vec_counts), 100 * len(vec_counts) / len(words)))\n",
        "        return embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t5tNfW29wqqj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "db00fa75-afec-4d81-9560-c5a460c73fab"
      },
      "cell_type": "code",
      "source": [
        "#There are 12716 distinct words in the entire vocabulary of passage+question+choice\n",
        "#The Glove embedding matrix will be 12716 x 300, the additional 12716-12614=102 vectors is zero padded\n",
        "#If the additional zero padded 102 x 300 dimension vector is not added, model.fit_generator gives error\n",
        "embedding_dim=300\n",
        "i=0\n",
        "embedding_file = \"./glove.840B.300d.txt\"\n",
        "glove_emb_matrix = []\n",
        "\n",
        "glove_emb_dict = load_embeddings(vocab.tokens(), embedding_file)\n",
        "\n",
        "pad_zeros_len = len(vocab.tokens()) - len(glove_emb_dict)\n",
        "\n",
        "embedding_zeros = np.zeros((pad_zeros_len, embedding_dim))\n",
        "print(np.shape(embedding_zeros))\n",
        "\n",
        "for k, v in glove_emb_dict.items():\n",
        "    glove_emb_matrix.append(v)\n",
        "\n",
        "print(np.shape(glove_emb_matrix))\n",
        "glove_emb_matrix = np.concatenate((np.array(glove_emb_matrix),embedding_zeros),axis=0)\n",
        "print(np.shape(glove_emb_matrix))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading pre-trained embeddings for 12716 words from ./glove.840B.300d.txt\n",
            "WARN: Duplicate embedding found for ;\n",
            "Loaded 12614 embeddings (99.20%)\n",
            "(102, 300)\n",
            "(12614, 300)\n",
            "(12716, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bktFYxWW0Igv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f961f39c-f65f-46b2-9fb3-177f03e16888"
      },
      "cell_type": "code",
      "source": [
        "print(len(vocab.tokens()), np.shape(glove_emb_matrix))\n",
        "print(len(pos_vocab.tokens()))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12716 (12716, 300)\n",
            "49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BSLxs1OiU8I2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Custom Data Generator"
      ]
    },
    {
      "metadata": {
        "id": "01biZ6_s1ult",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _to_indices_and_mask(batch_tensor, mask_type, need_mask=True):\n",
        "    #print(batch_tensor[0])\n",
        "    #mx_len = max([len(t) for t in batch_tensor])\n",
        "    if mask_type == 'p':\n",
        "        mx_len = MAX_PARA_LENGTH\n",
        "    elif mask_type == 'q':\n",
        "        mx_len = MAX_QUESTION_LENGTH\n",
        "    elif mask_type == 'c':\n",
        "        mx_len = MAX_CHOICE_LENGTH\n",
        "   \n",
        "    batch_size = len(batch_tensor)\n",
        "    \n",
        "    indices = np.zeros((batch_size,mx_len))\n",
        "\n",
        "    if need_mask:\n",
        "        mask = np.zeros((batch_size,mx_len))\n",
        "    \n",
        "    for i, t in enumerate(batch_tensor):\n",
        "        indices[i, :len(t)] = t\n",
        "        #This creates a mask of ones if the word is present, and zeros for the remaining part of the shorter sequence\n",
        "        #If a question is say 10 words long, then the mask will be [1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0... and so on upto 25]\n",
        "        if need_mask:\n",
        "            mask[i, :len(t)] = np.ones((len(t)))\n",
        "\n",
        "    if need_mask:\n",
        "        return indices, mask\n",
        "    else:\n",
        "        return indices\n",
        "\n",
        "def _to_feature_tensor(features):\n",
        "    #mx_len = max([len(f) for f in features])\n",
        "    mx_len = MAX_PARA_LENGTH\n",
        "    batch_size = len(features)\n",
        "    #f_dim is 5 for the handcrafted features\n",
        "    f_dim = len(features[0][1])\n",
        "    \n",
        "    f_tensor = np.zeros((batch_size, mx_len, f_dim))\n",
        "    for i, f in enumerate(features):\n",
        "        f_tensor[i, :len(f), :] = f\n",
        "    return f_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zv0MjlOf1wtA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batchify(batch_data):\n",
        "    #print(\"#####{}\".format(batch_data[0].d_tensor))\n",
        "    #x=[ex.d_tensor for ex in batch_data]\n",
        "    #print(x)\n",
        "    #print(type(x))\n",
        "    #p, p_mask = _to_indices_and_mask([ex.d_tensor for ex in batch_data])\n",
        "    p, p_mask = _to_indices_and_mask([ex.d_tensor for ex in batch_data],mask_type='p')\n",
        "    p_pos = _to_indices_and_mask([ex.d_pos_tensor for ex in batch_data], need_mask=False, mask_type='p')\n",
        "    p_ner = _to_indices_and_mask([ex.d_ner_tensor for ex in batch_data], need_mask=False, mask_type='p')\n",
        "    p_q_relation = _to_indices_and_mask([ex.p_q_relation for ex in batch_data], need_mask=False, mask_type='p')\n",
        "    p_c_relation = _to_indices_and_mask([ex.p_c_relation for ex in batch_data], need_mask=False, mask_type='p')\n",
        "    q, q_mask = _to_indices_and_mask([ex.q_tensor for ex in batch_data], mask_type='q')\n",
        "    q_pos = _to_indices_and_mask([ex.q_pos_tensor for ex in batch_data], need_mask=False, mask_type='q')\n",
        "    choices = [ex.choice.split() for ex in batch_data]\n",
        "    c, c_mask = _to_indices_and_mask([ex.c_tensor for ex in batch_data], mask_type='c')\n",
        "    f_tensor = _to_feature_tensor([ex.features for ex in batch_data])\n",
        "    y = [ex.label for ex in batch_data]\n",
        "    #It is necessary for custom generator to have format as below\n",
        "    #Multiple inputs should be a list of numpy arrays, inside a tuple for eg. ([list of numpy array inputs],numpy array output label)\n",
        "    return [np.array(p), np.array(p_mask), np.array(p_pos), np.array(p_ner), np.array(q), np.array(q_mask), \\\n",
        "            np.array(q_pos), np.array(c), np.array(c_mask), np.array(f_tensor), np.array(p_q_relation), \\\n",
        "            np.array(p_c_relation)],np.array(y) \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UanYqbui1zb_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _iter_data(data):\n",
        "    #Below while true is needed for custom data generator when called by model.fit_generator\n",
        "    #Else model.fit_generator will throw StopIteration Error\n",
        "    #When testing using model.fit, while true can be commented out\n",
        "    while True:\n",
        "        #print(type(data[0]))\n",
        "        #print(data[0])\n",
        "        num_iter = (len(data) + batch_size - 1) // batch_size\n",
        "        #print(len(data))\n",
        "        for i in range(num_iter):\n",
        "            start_idx = i * batch_size\n",
        "            batch_data = data[start_idx:(start_idx + batch_size)]\n",
        "            #print(i)\n",
        "            batch_input = batchify(batch_data)\n",
        "            \n",
        "            #Use yield when using model.fit_generator, \n",
        "            #Use return statement for manual testing/model.fit\n",
        "            #Ensure yield is within the for loop\n",
        "            yield batch_input\n",
        "        #return batch_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iAAO9Nz612cJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "cc8e8881-7daa-492b-dd28-cbea9683b1ef"
      },
      "cell_type": "code",
      "source": [
        "#iter_cnt, num_iter = 0, (len(train_data) + batch_size - 1) // batch_size\n",
        "#for batch_input in _iter_data(train_data):\n",
        "#        feed_input = [x for x in batch_input[:-1]]\n",
        "#        y = batch_input[-1]\n",
        "def data_generator(data):\n",
        "    iter_cnt, num_iter = 0, (len(trial_data) + batch_size - 1) // batch_size\n",
        "    for batch_input, y in _iter_data(data):\n",
        "        feed_input = [x for x in batch_input[:-1]]\n",
        "        y = batch_input[-1]\n",
        "    return (feed_input, y)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OTyADXcC16lZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5cbd6332-29cc-4b7c-d7bd-a46ccac5152b"
      },
      "cell_type": "code",
      "source": [
        "#When calling below statement for manual testing/when using model.fit, ensure _iter_data has return statement and not yield\n",
        "(feed_input,y) = _iter_data(trial_data)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xjw3q1VqTn3H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Model\n"
      ]
    },
    {
      "metadata": {
        "id": "UanoZBlITtKY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Attention Layers\n",
        "Build custom Keras layer"
      ]
    },
    {
      "metadata": {
        "id": "EyleUPKpww42",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SeqAttLayer(Layer):\n",
        "\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        super(SeqAttLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        self.kernel = self.add_weight(name='kernel', \\\n",
        "                                      shape=(self.output_dim,input_shape[0][1]),\\\n",
        "                                      initializer='uniform',\\\n",
        "                                      trainable=True)\n",
        "        super(SeqAttLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        x_proj = K.relu(K.dot(x[0],self.kernel))\n",
        "        y_proj = K.relu(K.dot(x[1],self.kernel))\n",
        "        \n",
        "        #For every word in the passage generate mapping for every word in the question\n",
        "        #For row 1, if question is 900 words x 300 dim, and associated question is 25 words x 300 dim\n",
        "        #Generate 900 x 25, ie match each word in the question with each word in the paassage\n",
        "        # passage(32 x 900 x 300) * question(32 x 25 x 300) \n",
        "        # (32 x 900 x 300) matmul (32 x 300 x 25) = (32 x 900 x 25) \n",
        "        # where rows=7=32, passage=900 words each of 300 vector size, question=25 words each of 300 vector size\n",
        "        scores = K.tf.matmul(x_proj,K.permute_dimensions(y_proj,(0,2,1)))\n",
        "        \n",
        "        #Apply the question mask to every word in the passage\n",
        "        #Creates 32 x 900 x 25 mask for the 900 words in the passage\n",
        "        y_mask = x[2]\n",
        "        y_mask = K.repeat(y_mask, K.int_shape(scores)[1])\n",
        "        #When the mask is multiplied by the scores, (max question length(25) - actual length of the question(say 10)) are filled with zeros\n",
        "        scores = K.tf.multiply(scores,y_mask)\n",
        "\n",
        "        # Normalize with softmax\n",
        "        alpha_flat = K.softmax(K.reshape(scores,(-1,K.int_shape(scores)[2])))\n",
        "        alpha = K.reshape(alpha_flat,(-1, K.int_shape(scores)[1],K.int_shape(scores)[2]))\n",
        "        \n",
        "        # Take weighted average\n",
        "        # (32 x 900 x 25) * (32 x 25 x 300) = (32 x 900 x 300)        \n",
        "        matched_seq = K.tf.matmul(alpha, x[1])\n",
        "        \n",
        "        return matched_seq\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wsq3dSmDwx-N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LinearSeqAttLayer(Layer):\n",
        "\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        super(LinearSeqAttLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        self.kernel = self.add_weight(name='kernel', \n",
        "                                      shape=(self.output_dim,1),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)\n",
        "        super(LinearSeqAttLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        #(32 x 35 x 192) => (1120 x 192)\n",
        "        x_flat = K.reshape(x[0],(-1,K.int_shape(x[0])[2]))\n",
        "\n",
        "        #(1120 x 192)  => (1120 x 1) => (32 x 35)\n",
        "        scores = K.reshape(K.dot(x_flat,self.kernel),(-1,K.int_shape(x[0])[1]))\n",
        "\n",
        "        y_mask = x[1]\n",
        "        scores = K.tf.multiply(scores,y_mask)\n",
        "        alpha = K.softmax(scores)\n",
        "        #(32 x 1 x 35) matmul (32 x 35 x 192) = (32 x 1 x 192) => (32 x 192)\n",
        "        alpha_hidden = K.squeeze(K.tf.matmul(K.expand_dims(alpha, axis=1),x[0]),axis=1)\n",
        "        #print(alpha_hidden)\n",
        "        \n",
        "        #return alpha\n",
        "        return alpha_hidden\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0][0], self.output_dim)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "959SN4kAw02j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BiLinearSeqAttLayer(Layer):\n",
        "\n",
        "    def __init__(self, output_dim, normalize=True, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        self.normalize = normalize\n",
        "        super(BiLinearSeqAttLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        self.kernel = self.add_weight(name='kernel', \n",
        "                                      shape=(self.output_dim,self.output_dim),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)\n",
        "        super(BiLinearSeqAttLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        \n",
        "        Wy = K.dot(x[1],self.kernel)\n",
        "        \n",
        "        #(32 x 900 x 192) matmul (32 x 192 x 1) => (32 x 900 x 1) => (32 x 900)\n",
        "        xWy = K.squeeze(K.tf.matmul(x[0],K.expand_dims(Wy,axis=2)), axis=2)\n",
        "\n",
        "        y_mask = x[2]\n",
        "        xWy = K.tf.multiply(xWy,y_mask)\n",
        "        \n",
        "        if self.normalize:\n",
        "            alpha = K.softmax(xWy)\n",
        "        else:\n",
        "            alpha = K.exp(xWy)\n",
        "         \n",
        "        #(32 x 1 x 900) matmul (32 x 900 x 192) => (32 x 1 x 192) => (32 x 192)\n",
        "        alpha_hidden = K.squeeze(K.tf.matmul(K.expand_dims(alpha, axis=1),x[0]),axis=1)\n",
        "\n",
        "        #return alpha\n",
        "        return alpha_hidden\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0][0], self.output_dim)       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ungclcMT3i1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Common Sense Core Layers and Model"
      ]
    },
    {
      "metadata": {
        "id": "gJ0fv--gw7Ds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ac60ab33-99cc-4b96-d942-82a09f3617a5"
      },
      "cell_type": "code",
      "source": [
        "embedding_dim=300\n",
        "pos_emb_dim=12\n",
        "ner_emb_dim=8\n",
        "rel_emb_dim=10\n",
        "dropout_rate = 0.4\n",
        "hidden_size=96\n",
        "        \n",
        "output_dim = 1\n",
        "p_inp = Input(shape=(MAX_PARA_LENGTH,))\n",
        "p_mask_inp = Input(shape=(MAX_PARA_LENGTH,))\n",
        "p_pos_inp = Input(shape=(MAX_PARA_LENGTH,))\n",
        "p_ner_inp = Input(shape=(MAX_PARA_LENGTH,))\n",
        "q_inp = Input(shape=(MAX_QUESTION_LENGTH,))\n",
        "q_mask_inp = Input(shape=(MAX_QUESTION_LENGTH,))\n",
        "q_pos_inp = Input(shape=(MAX_QUESTION_LENGTH,))\n",
        "c_inp = Input(shape=(MAX_CHOICE_LENGTH,))\n",
        "c_mask_inp = Input(shape=(MAX_CHOICE_LENGTH,))\n",
        "f_tensor_inp = Input(shape=(MAX_PARA_LENGTH,5))\n",
        "p_q_rel_inp = Input(shape=(MAX_PARA_LENGTH,))\n",
        "p_c_rel_inp = Input(shape=(MAX_PARA_LENGTH,))\n",
        "\n",
        "#glov_emb = Embedding(len(vocab.tokens()), embedding_dim, embeddings_initializer=TruncatedNormal(mean=0.0, stddev=0.1))\n",
        "#Loading the Glove embeddings, ensure trainable=False\n",
        "glov_emb = Embedding(len(vocab.tokens()), embedding_dim, weights=[glove_emb_matrix], trainable=False)\n",
        "pos_emb = Embedding(len(pos_vocab.tokens()), pos_emb_dim, embeddings_initializer=TruncatedNormal(mean=0.0, stddev=0.1))\n",
        "ner_emb = Embedding(len(ner_vocab.tokens()), ner_emb_dim, embeddings_initializer=TruncatedNormal(mean=0.0, stddev=0.1))\n",
        "rel_emb = Embedding(len(rel_vocab.tokens()), rel_emb_dim, embeddings_initializer=TruncatedNormal(mean=0.0, stddev=0.1))\n",
        "       \n",
        "#p_emb:(32 x 900 x 300), q_emb:(32 x 25 x 300), c_emb:(32 x 35 x 300), q_mask_inp:(32 x 25)  \n",
        "p_emb, q_emb, c_emb = glov_emb(p_inp), glov_emb(q_inp), glov_emb(c_inp)\n",
        "#p_pos_emb:(32 x 900 x 12), p_ner_emb:(32 x 900 x 8), q_pos_emb:(32 x 25 x 12)\n",
        "p_pos_emb, p_ner_emb, q_pos_emb = pos_emb(p_pos_inp), ner_emb(p_ner_inp), pos_emb(q_pos_inp)\n",
        "#p_q_rel_emb:(32 x 900 x 10), p_c_rel_emb:(32 x 900 x 10)\n",
        "p_q_rel_emb, p_c_rel_emb = rel_emb(p_q_rel_inp), rel_emb(p_c_rel_inp)\n",
        "\n",
        "\n",
        "p_emb = Dropout(dropout_rate)(p_emb)\n",
        "q_emb = Dropout(dropout_rate)(q_emb)\n",
        "c_emb = Dropout(dropout_rate)(c_emb)\n",
        "p_pos_emb = Dropout(dropout_rate)(p_pos_emb)\n",
        "p_ner_emb = Dropout(dropout_rate)(p_ner_emb)\n",
        "q_pos_emb = Dropout(dropout_rate)(q_pos_emb)\n",
        "p_q_rel_emb = Dropout(dropout_rate)(p_q_rel_emb)\n",
        "p_c_rel_emb = Dropout(dropout_rate)(p_c_rel_emb)\n",
        "\n",
        "#(32 x 900 x 300)\n",
        "p_q_weighted_emb = SeqAttLayer(embedding_dim)([p_emb,q_emb,q_mask_inp])\n",
        "#(32 x 35 x 300)\n",
        "c_q_weighted_emb = SeqAttLayer(embedding_dim)([c_emb,q_emb,q_mask_inp])\n",
        "#(32 x 35 x 300)\n",
        "c_p_weighted_emb = SeqAttLayer(embedding_dim)([c_emb,p_emb,p_mask_inp])\n",
        "\n",
        "p_q_weighted_emb = Dropout(dropout_rate)(p_q_weighted_emb)\n",
        "c_q_weighted_emb = Dropout(dropout_rate)(c_q_weighted_emb)\n",
        "c_p_weighted_emb = Dropout(dropout_rate)(c_p_weighted_emb)\n",
        "\n",
        "#(32x900x300)+(32x900x300)+(32x900x12)+(32x900x8)+(32x900x10)+(32x900x5)+(32x900x10)+(32x900x10) = (32x900x645)\n",
        "p_rnn_input = Concatenate(axis=-1)([p_emb, p_q_weighted_emb, p_pos_emb, p_ner_emb, f_tensor_inp, p_q_rel_emb, p_c_rel_emb])\n",
        "c_rnn_input = Concatenate(axis=-1)([c_emb, c_q_weighted_emb, c_p_weighted_emb])\n",
        "q_rnn_input = Concatenate(axis=-1)([q_emb, q_pos_emb])\n",
        "\n",
        "#(32 x 900 x 192)\n",
        "p_hiddens = Bidirectional(LSTM(hidden_size, return_sequences=True))([p_rnn_input])\n",
        "#(32 x 35 x 192)\n",
        "c_hiddens = Bidirectional(LSTM(hidden_size, return_sequences=True))([c_rnn_input])\n",
        "#(32 x 25 x 192)\n",
        "q_hiddens = Bidirectional(LSTM(hidden_size, return_sequences=True))([q_rnn_input])\n",
        "\n",
        "#c_merge_weights = LinearSeqAttLayer(2 * hidden_size)([c_hiddens, c_mask_inp])\n",
        "#c_hidden = K.squeeze(K.tf.matmul(K.expand_dims(c_merge_weights, axis=1),c_hiddens),axis=1)\n",
        "#Input = (32 x 35 x 192) and (32 x 35)\n",
        "#Output = (32 x 192)\n",
        "c_hidden = LinearSeqAttLayer(2 * hidden_size)([c_hiddens, c_mask_inp])\n",
        "\n",
        "#q_merge_weights = LinearSeqAttLayer(2 * hidden_size)([q_hiddens, q_mask_inp])\n",
        "#q_hidden = K.squeeze(K.tf.matmul(K.expand_dims(q_merge_weights, axis=1),q_hiddens),axis=1)\n",
        "#Input = (32 x 25 x 192) and (32 x 25)\n",
        "#Output = (32 x 192)\n",
        "q_hidden = LinearSeqAttLayer(2 * hidden_size)([q_hiddens, q_mask_inp])\n",
        "\n",
        "#p_merge_weights = BiLinearSeqAttLayer(2 * hidden_size)([p_hiddens, q_hidden, p_mask_inp])\n",
        "#p_hidden = K.squeeze(K.tf.matmul(K.expand_dims(p_merge_weights, axis=1),p_hiddens),axis=1)\n",
        "#Input = (32 x 900 x 192), (32 x 192), (32 x 900)\n",
        "#Output = (32 x 192)\n",
        "p_hidden = BiLinearSeqAttLayer(2 * hidden_size)([p_hiddens, q_hidden, p_mask_inp])\n",
        "\n",
        "#(32 x 192)\n",
        "logits1_mul = Multiply()([Dense(2 * hidden_size)(p_hidden), c_hidden])\n",
        "#(32 x 1)\n",
        "logits1 = Reshape((1,))(Lambda(lambda x: K.sum(x,axis=1))(logits1_mul))\n",
        "\n",
        "#(32 x 192)\n",
        "logits2_mul = Multiply()([Dense(2 * hidden_size)(q_hidden), c_hidden])\n",
        "#(32 x 1)\n",
        "logits2 = Reshape((1,))(Lambda(lambda x: K.sum(x,axis=1))(logits1_mul))\n",
        "\n",
        "#(32 x 1)\n",
        "preds = Activation('sigmoid')(Add()([logits1,logits2]))\n",
        "\n",
        "print(logits1, logits2, preds)\n",
        "model = Model(inputs=[p_inp,p_mask_inp,p_pos_inp,p_ner_inp,q_inp,q_mask_inp,q_pos_inp,c_inp,c_mask_inp,f_tensor_inp,p_q_rel_inp,p_c_rel_inp], outputs=preds)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"reshape_1/Reshape:0\", shape=(?, 1), dtype=float32) Tensor(\"reshape_2/Reshape:0\", shape=(?, 1), dtype=float32) Tensor(\"activation_1/Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xg5DOT3eUMKI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Compile Model"
      ]
    },
    {
      "metadata": {
        "id": "QFy4WX4T5gAV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2154
        },
        "outputId": "54ae467e-0dfb-4753-c42e-4db0f0a07473"
      },
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "adamax = Adamax(lr=2e-3)\n",
        "model.compile(loss='binary_crossentropy',optimizer=adamax,metrics=['acc'])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_25 (InputLayer)           (None, 900)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_29 (InputLayer)           (None, 25)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_9 (Embedding)         multiple             3814800     input_25[0][0]                   \n",
            "                                                                 input_29[0][0]                   \n",
            "                                                                 input_32[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_27 (InputLayer)           (None, 900)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_31 (InputLayer)           (None, 25)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_32 (InputLayer)           (None, 35)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 900, 300)     0           embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 25, 300)      0           embedding_9[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "input_30 (InputLayer)           (None, 25)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_10 (Embedding)        multiple             588         input_27[0][0]                   \n",
            "                                                                 input_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_28 (InputLayer)           (None, 900)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_35 (InputLayer)           (None, 900)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_36 (InputLayer)           (None, 900)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "seq_att_layer_1 (SeqAttLayer)   (None, 900, 300)     270000      dropout_1[0][0]                  \n",
            "                                                                 dropout_2[0][0]                  \n",
            "                                                                 input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_11 (Embedding)        (None, 900, 8)       144         input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_12 (Embedding)        (None, 900, 10)      370         input_35[0][0]                   \n",
            "                                                                 input_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 25, 12)       0           embedding_10[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "input_26 (InputLayer)           (None, 900)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 35, 300)      0           embedding_9[2][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 900, 300)     0           seq_att_layer_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 900, 12)      0           embedding_10[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 900, 8)       0           embedding_11[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "input_34 (InputLayer)           (None, 900, 5)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 900, 10)      0           embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 900, 10)      0           embedding_12[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 25, 312)      0           dropout_2[0][0]                  \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "seq_att_layer_2 (SeqAttLayer)   (None, 35, 300)      10500       dropout_3[0][0]                  \n",
            "                                                                 dropout_2[0][0]                  \n",
            "                                                                 input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "seq_att_layer_3 (SeqAttLayer)   (None, 35, 300)      10500       dropout_3[0][0]                  \n",
            "                                                                 dropout_1[0][0]                  \n",
            "                                                                 input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 900, 645)     0           dropout_1[0][0]                  \n",
            "                                                                 dropout_9[0][0]                  \n",
            "                                                                 dropout_4[0][0]                  \n",
            "                                                                 dropout_5[0][0]                  \n",
            "                                                                 input_34[0][0]                   \n",
            "                                                                 dropout_7[0][0]                  \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 25, 192)      314112      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 35, 300)      0           seq_att_layer_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 35, 300)      0           seq_att_layer_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 900, 192)     569856      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "linear_seq_att_layer_2 (LinearS (None, 192)          192         bidirectional_3[0][0]            \n",
            "                                                                 input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 35, 900)      0           dropout_3[0][0]                  \n",
            "                                                                 dropout_10[0][0]                 \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bi_linear_seq_att_layer_1 (BiLi (None, 192)          36864       bidirectional_1[0][0]            \n",
            "                                                                 linear_seq_att_layer_2[0][0]     \n",
            "                                                                 input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 35, 192)      765696      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_33 (InputLayer)           (None, 35)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 192)          37056       bi_linear_seq_att_layer_1[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "linear_seq_att_layer_1 (LinearS (None, 192)          192         bidirectional_2[0][0]            \n",
            "                                                                 input_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 192)          0           dense_1[0][0]                    \n",
            "                                                                 linear_seq_att_layer_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None,)              0           multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None,)              0           multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 1)            0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 1)            0           lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 1)            0           reshape_1[0][0]                  \n",
            "                                                                 reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 1)            0           add_1[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 5,830,870\n",
            "Trainable params: 2,016,070\n",
            "Non-trainable params: 3,814,800\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6WyLefpUUXb3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Train Model"
      ]
    },
    {
      "metadata": {
        "id": "n3uhUZP25huv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "36b4bb6c-4f89-4d88-88f8-76bdc0b4b186"
      },
      "cell_type": "code",
      "source": [
        "#model.evaluate(feed_input, y)\n",
        "#model.fit(feed_input, y, batch_size=batch_size, epochs=2) \n",
        "\n",
        "#19462/32=608, so chose 600 steps\n",
        "model.fit_generator(generator=_iter_data(train_data), steps_per_epoch=600, epochs=10, verbose=1, callbacks=None, \\\n",
        "                    validation_data=_iter_data(dev_data), validation_steps=600)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "600/600 [==============================] - 3645s 6s/step - loss: 0.7097 - acc: 0.5347 - val_loss: 0.6820 - val_acc: 0.5483\n",
            "Epoch 2/10\n",
            "194/600 [========>.....................] - ETA: 33:30 - loss: 0.6801 - acc: 0.5564"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "600/600 [==============================] - 3628s 6s/step - loss: 0.6705 - acc: 0.5787 - val_loss: 0.6595 - val_acc: 0.5758\n",
            "Epoch 3/10\n",
            "308/600 [==============>...............] - ETA: 23:47 - loss: 0.6506 - acc: 0.6161"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "600/600 [==============================] - 3615s 6s/step - loss: 0.6507 - acc: 0.6131 - val_loss: 0.6500 - val_acc: 0.6122\n",
            "Epoch 4/10\n",
            "351/600 [================>.............] - ETA: 20:39 - loss: 0.6433 - acc: 0.6220"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "600/600 [==============================] - 3691s 6s/step - loss: 0.6428 - acc: 0.6208 - val_loss: 0.6459 - val_acc: 0.6138\n",
            "Epoch 5/10\n",
            "260/600 [============>.................] - ETA: 27:57 - loss: 0.6349 - acc: 0.6298"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K-TbDBsJ-V7g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Prediction"
      ]
    },
    {
      "metadata": {
        "id": "sDlsJiTcBWU_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.predict_generator(generator=_iter_data(test_data[:10]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7kH0PJU_sZqp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pending Action Items\n",
        "* Python generator should be an instance of keras.utils.Sequence, which guarantees ordering and single use of every input per epoch when using use_multiprocessing=True and also avoids duplicate data.\n",
        "* Model was not trained on RACE dataset for initial 10 epochs\n",
        "* CUDA was not used\n",
        "* Gradient Clipping was not used\n",
        "* Learning rate was not halved after 10 epochs, 50 epochs were not done\n",
        "* Finetune topk embeddings during training was not done\n",
        "* RNN padding and masking was not done"
      ]
    },
    {
      "metadata": {
        "id": "ugFv63Z9LDZD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "\n",
        "[CS231n Winter 2016: Lecture 10: Recurrent Neural Networks, Image Captioning, LSTM](https://www.youtube.com/watch?v=yCC09vCHzF8)\n",
        "\n",
        "[Evolution: from vanilla RNN to GRU & LSTMs (How it works)](https://www.youtube.com/watch?v=lycKqccytfU)\n",
        "\n",
        "[Keras LSTM tutorial – How to easily build a powerful deep learning language model](http://adventuresinmachinelearning.com/keras-lstm-tutorial/)\n",
        "\n",
        "[How to Develop a Bidirectional LSTM For Sequence Classification in Python with Keras](https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/)\n",
        "\n",
        "[When should one use bidirectional LSTM as opposed to normal LSTM?](https://www.quora.com/When-should-one-use-bidirectional-LSTM-as-opposed-to-normal-LSTM)\n",
        "\n",
        "[What is exactly the attention mechanism introduced to RNN (recurrent neural network)? It would be nice if you could make it easy to understand!](https://www.quora.com/What-is-exactly-the-attention-mechanism-introduced-to-RNN-recurrent-neural-network-It-would-be-nice-if-you-could-make-it-easy-to-understand)\n",
        "\n",
        "[A Brief Overview of Attention Mechanism](https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129)\n",
        "\n",
        "[Keras Attention Mechanism](https://github.com/philipperemy/keras-attention-mechanism)\n",
        "\n",
        "[How to add Attention on top of a Recurrent Layer (Text Classification)](https://github.com/keras-team/keras/issues/4962)\n",
        "\n",
        "[How Does Attention Work in Encoder-Decoder Recurrent Neural Networks](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)\n",
        "\n",
        "[Attention in Long Short-Term Memory Recurrent Neural Networks](https://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/)\n",
        "\n",
        "[Pytorch Documentation](https://pytorch.org/docs/master/nn.html)\n",
        "\n",
        "[Writing your own Keras Layer - Keras Documentation](https://keras.io/layers/writing-your-own-keras-layers/)\n",
        "\n",
        "[Keras Backend functions - Keras Documentation](https://keras.io/backend/)\n",
        "\n",
        "[Text Classification, Part 2 - sentence level Attentional RNN](https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-RNN/)\n",
        "\n",
        "[Tensorflow Documentation](https://www.tensorflow.org/api_docs/python/tf)\n",
        "\n",
        "[Can Keras deal with input images with different size?](https://stackoverflow.com/questions/39814777/can-keras-deal-with-input-images-with-different-size/41092113)\n",
        "\n",
        "[Pre-trained Word Embedding Using Keras - Github Code](https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)"
      ]
    }
  ]
}